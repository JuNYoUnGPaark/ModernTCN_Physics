{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67CYL0qGDnos",
        "outputId": "1305cfc8-07e3-44a2-a6f6-10b7b3518eec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Train: 7352 samples, Test: 2947 samples\n",
            "\n",
            "\n",
            "üìä Î™®Îç∏ Íµ¨Ï°∞:\n",
            "  - Multi-scale kernels: [3, 7] (ÏûëÏùÄ Ïª§ÎÑê + ÌÅ∞ Ïª§ÎÑê)\n",
            "  - Large kernel: 21 (Í∏ÄÎ°úÎ≤å Ïª®ÌÖçÏä§Ìä∏)\n",
            "  - Hidden dim: 128\n",
            "  - Layers: 4\n",
            "  - Squeeze-Excitation: Enabled\n",
            "  - Total parameters: 362,324\n",
            "\n",
            "==================================================\n",
            "üî• Physics-Guided (Multi-scale Modern TCN) HAR ÌïôÏäµ ÏãúÏûë\n",
            "==================================================\n",
            "[Physics] Epoch 10/50: Train Acc=96.67%, Test Acc=95.66% (Best: 95.69%)\n",
            "[Physics] Epoch 20/50: Train Acc=98.67%, Test Acc=97.39% (Best: 97.39%)\n",
            "[Physics] Epoch 30/50: Train Acc=99.59%, Test Acc=97.35% (Best: 97.69%)\n",
            "[Physics] Epoch 40/50: Train Acc=99.76%, Test Acc=97.01% (Best: 97.69%)\n",
            "[Physics] Epoch 50/50: Train Acc=99.82%, Test Acc=96.81% (Best: 97.69%)\n",
            "\n",
            "==================================================\n",
            "FINAL RESULT\n",
            "==================================================\n",
            "üî• Physics-Guided Multi-scale Modern TCN HAR (Best Test Acc): 97.69%\n",
            "   - Small kernels (3, 7) for local patterns\n",
            "   - Large kernel (21) for global context\n",
            "   - Physics-guided learning with auxiliary task\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pathlib import Path\n",
        "import time\n",
        "from collections import deque\n",
        "\n",
        "# ========================\n",
        "# UCI-HAR Îç∞Ïù¥ÌÑ∞ Î°úÎìú\n",
        "# ========================\n",
        "class UCIHARDataset(Dataset):\n",
        "    def __init__(self, data_path, split='train'):\n",
        "        base = Path(data_path) / split\n",
        "        signals = []\n",
        "        for sensor in ['body_acc', 'body_gyro', 'total_acc']:\n",
        "            for axis in ['x', 'y', 'z']:\n",
        "                file = base / 'Inertial Signals' / f'{sensor}_{axis}_{split}.txt'\n",
        "                signals.append(np.loadtxt(file))\n",
        "\n",
        "        self.X = np.stack(signals, axis=-1)\n",
        "        self.y = np.loadtxt(base.parent / split / f'y_{split}.txt').astype(int) - 1\n",
        "\n",
        "        try:\n",
        "            self.subjects = np.loadtxt(base.parent / split / f'subject_{split}.txt').astype(int)\n",
        "        except:\n",
        "            self.subjects = np.ones(len(self.y))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (torch.FloatTensor(self.X[idx]),\n",
        "                torch.LongTensor([self.y[idx]])[0],\n",
        "                self.subjects[idx])\n",
        "\n",
        "# ========================\n",
        "# üî• Modern TCN Components\n",
        "# ========================\n",
        "class DepthwiseSeparableConv1d(nn.Module):\n",
        "    \"\"\"Depthwise Separable Convolution for efficiency\"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, dilation=1, padding=0):\n",
        "        super().__init__()\n",
        "        self.depthwise = nn.Conv1d(\n",
        "            in_channels, in_channels, kernel_size,\n",
        "            padding=padding, dilation=dilation, groups=in_channels\n",
        "        )\n",
        "        self.pointwise = nn.Conv1d(in_channels, out_channels, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.depthwise(x)\n",
        "        x = self.pointwise(x)\n",
        "        return x\n",
        "\n",
        "class MultiScaleConvBlock(nn.Module):\n",
        "    \"\"\"Multi-scale convolution block with small and large kernels\"\"\"\n",
        "    def __init__(self, channels, kernel_sizes=[3, 5, 7], dilation=1, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.branches = nn.ModuleList()\n",
        "        for k in kernel_sizes:\n",
        "            # Use 'same' padding to maintain length\n",
        "            padding = ((k - 1) * dilation) // 2\n",
        "            branch = nn.ModuleDict({\n",
        "                'conv': DepthwiseSeparableConv1d(channels, channels, k, dilation, padding),\n",
        "                'norm': nn.BatchNorm1d(channels),  # Use BatchNorm1d\n",
        "                'dropout': nn.Dropout(dropout)\n",
        "            })\n",
        "            self.branches.append(branch)\n",
        "\n",
        "        # Fusion layer\n",
        "        self.fusion = nn.Conv1d(channels * len(kernel_sizes), channels, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B, C, L]\n",
        "        outputs = []\n",
        "        target_length = x.size(2)  # Store original length\n",
        "\n",
        "        for branch in self.branches:\n",
        "            out = branch['conv'](x)\n",
        "            # Ensure all outputs have the same length\n",
        "            if out.size(2) != target_length:\n",
        "                out = out[:, :, :target_length]\n",
        "            out = branch['norm'](out)  # BatchNorm1d works with [B, C, L]\n",
        "            out = F.gelu(out)\n",
        "            out = branch['dropout'](out)\n",
        "            outputs.append(out)\n",
        "\n",
        "        # Concatenate and fuse\n",
        "        multi_scale = torch.cat(outputs, dim=1)\n",
        "        return self.fusion(multi_scale)\n",
        "\n",
        "class ModernTCNBlock(nn.Module):\n",
        "    \"\"\"Modern TCN Block with:\n",
        "    - Multi-scale convolutions (small and large kernels)\n",
        "    - Batch Normalization (for Conv1d compatibility)\n",
        "    - GELU activation\n",
        "    - Residual connections\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel_sizes=[3, 7], dilation=1, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        # First multi-scale conv\n",
        "        self.multi_conv1 = MultiScaleConvBlock(\n",
        "            in_channels if in_channels == out_channels else out_channels,\n",
        "            kernel_sizes, dilation, dropout\n",
        "        )\n",
        "\n",
        "        # Standard depthwise separable conv\n",
        "        max_k = max(kernel_sizes)\n",
        "        padding = ((max_k - 1) * dilation) // 2\n",
        "        self.conv2 = DepthwiseSeparableConv1d(\n",
        "            out_channels, out_channels, max_k, dilation, padding\n",
        "        )\n",
        "        self.norm2 = nn.BatchNorm1d(out_channels)  # Use BatchNorm1d\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "        # Residual connection\n",
        "        self.downsample = nn.Conv1d(in_channels, out_channels, 1) if in_channels != out_channels else None\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B, C, L]\n",
        "        residual = x\n",
        "        target_length = x.size(2)\n",
        "\n",
        "        # Adjust channels if needed\n",
        "        if self.downsample is not None:\n",
        "            x = self.downsample(x)\n",
        "            residual = x\n",
        "\n",
        "        # Multi-scale convolution\n",
        "        out = self.multi_conv1(x)\n",
        "        if out.size(2) != target_length:\n",
        "            out = out[:, :, :target_length]\n",
        "\n",
        "        # Standard convolution\n",
        "        out = self.conv2(out)\n",
        "        if out.size(2) != target_length:\n",
        "            out = out[:, :, :target_length]\n",
        "        out = self.norm2(out)  # BatchNorm1d works with [B, C, L]\n",
        "        out = F.gelu(out)\n",
        "        out = self.dropout2(out)\n",
        "\n",
        "        # Residual connection\n",
        "        return F.gelu(out + residual)\n",
        "\n",
        "class SqueezeExcitation1d(nn.Module):\n",
        "    \"\"\"Squeeze-and-Excitation block for channel attention\"\"\"\n",
        "    def __init__(self, channels, reduction=16):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(channels, channels // reduction)\n",
        "        self.fc2 = nn.Linear(channels // reduction, channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B, C, L]\n",
        "        batch, channels, _ = x.size()\n",
        "\n",
        "        # Squeeze: Global Average Pooling\n",
        "        squeeze = F.adaptive_avg_pool1d(x, 1).view(batch, channels)\n",
        "\n",
        "        # Excitation\n",
        "        excitation = F.relu(self.fc1(squeeze))\n",
        "        excitation = torch.sigmoid(self.fc2(excitation)).view(batch, channels, 1)\n",
        "\n",
        "        return x * excitation\n",
        "\n",
        "class LargeKernelConv1d(nn.Module):\n",
        "    \"\"\"Large kernel convolution decomposed into depthwise and pointwise\"\"\"\n",
        "    def __init__(self, channels, kernel_size=21):\n",
        "        super().__init__()\n",
        "        padding = kernel_size // 2\n",
        "        self.depthwise = nn.Conv1d(\n",
        "            channels, channels, kernel_size,\n",
        "            padding=padding, groups=channels\n",
        "        )\n",
        "        self.norm = nn.BatchNorm1d(channels)  # Use BatchNorm1d instead of LayerNorm\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B, C, L]\n",
        "        out = self.depthwise(x)\n",
        "        out = self.norm(out)  # BatchNorm1d works with [B, C, L]\n",
        "        return out\n",
        "\n",
        "# ========================\n",
        "# Modern TCN Base Î™®Îç∏ (Multi-scale)\n",
        "# ========================\n",
        "class BaseModernTCNHAR(nn.Module):\n",
        "    def __init__(self, input_dim=9, hidden_dim=128, n_layers=4, n_classes=6,\n",
        "                 kernel_sizes=[3, 7], large_kernel=21, dropout=0.1, use_se=True):\n",
        "        super().__init__()\n",
        "\n",
        "        # Input projection\n",
        "        self.input_proj = nn.Conv1d(input_dim, hidden_dim, 1)\n",
        "\n",
        "        # Large kernel at the beginning for global context\n",
        "        self.large_kernel_conv = LargeKernelConv1d(hidden_dim, large_kernel)\n",
        "\n",
        "        # TCN blocks with exponentially increasing dilation and multi-scale kernels\n",
        "        self.tcn_blocks = nn.ModuleList()\n",
        "        for i in range(n_layers):\n",
        "            dilation = 2 ** i\n",
        "            self.tcn_blocks.append(\n",
        "                ModernTCNBlock(\n",
        "                    hidden_dim, hidden_dim,\n",
        "                    kernel_sizes=kernel_sizes,\n",
        "                    dilation=dilation,\n",
        "                    dropout=dropout\n",
        "                )\n",
        "            )\n",
        "\n",
        "        # Additional large kernel at the end for global aggregation\n",
        "        self.final_large_kernel = LargeKernelConv1d(hidden_dim, large_kernel)\n",
        "\n",
        "        # Optional Squeeze-and-Excitation\n",
        "        self.use_se = use_se\n",
        "        if use_se:\n",
        "            self.se = SqueezeExcitation1d(hidden_dim)\n",
        "\n",
        "        # Global pooling and classification head\n",
        "        self.norm_final = nn.LayerNorm(hidden_dim)\n",
        "        self.head = nn.Linear(hidden_dim, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [B, L, C] -> [B, C, L] for Conv1d\n",
        "        x = x.transpose(1, 2)\n",
        "\n",
        "        # Input projection\n",
        "        x = self.input_proj(x)\n",
        "\n",
        "        # Initial large kernel for global context\n",
        "        x = self.large_kernel_conv(x)\n",
        "        x = F.gelu(x)\n",
        "\n",
        "        # TCN blocks with multi-scale kernels\n",
        "        for block in self.tcn_blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        # Final large kernel for global aggregation\n",
        "        x = self.final_large_kernel(x)\n",
        "        x = F.gelu(x)\n",
        "\n",
        "        # Squeeze-and-Excitation\n",
        "        if self.use_se:\n",
        "            x = self.se(x)\n",
        "\n",
        "        # Global average pooling\n",
        "        x = F.adaptive_avg_pool1d(x, 1).squeeze(-1)  # [B, C]\n",
        "\n",
        "        # Layer norm and classification\n",
        "        x = self.norm_final(x)\n",
        "        return self.head(x)\n",
        "\n",
        "# ========================\n",
        "# Physics-Guided Modern TCN HAR\n",
        "# ========================\n",
        "class PhysicsModernTCNHAR(BaseModernTCNHAR):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "        hidden_dim = self.head.in_features\n",
        "        # Î≥¥Ï°∞ ÌÉúÏä§ÌÅ¨Î•º ÏúÑÌïú 'Î¨ºÎ¶¨ Ìó§Îìú' Ï∂îÍ∞Ä\n",
        "        self.physics_head = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim // 2, 6)  # 6 = acc (x,y,z) + gyro (x,y,z)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, return_physics=False):\n",
        "        # x: [B, L, C] -> [B, C, L] for Conv1d\n",
        "        x = x.transpose(1, 2)\n",
        "\n",
        "        # Input projection\n",
        "        x_feat = self.input_proj(x)\n",
        "\n",
        "        # Initial large kernel\n",
        "        x_feat = self.large_kernel_conv(x_feat)\n",
        "        x_feat = F.gelu(x_feat)\n",
        "\n",
        "        # TCN blocks\n",
        "        for block in self.tcn_blocks:\n",
        "            x_feat = block(x_feat)\n",
        "\n",
        "        # Final large kernel\n",
        "        x_feat = self.final_large_kernel(x_feat)\n",
        "        x_feat = F.gelu(x_feat)\n",
        "\n",
        "        # Squeeze-and-Excitation\n",
        "        if self.use_se:\n",
        "            x_feat = self.se(x_feat)\n",
        "\n",
        "        # x_feat: [B, C, L]\n",
        "\n",
        "        # 1. Î∂ÑÎ•ò Ìó§Îìú (Í∏∞Î≥∏ ÌÉúÏä§ÌÅ¨)\n",
        "        pooled = F.adaptive_avg_pool1d(x_feat, 1).squeeze(-1)  # [B, C]\n",
        "        pooled = self.norm_final(pooled)\n",
        "        logits = self.head(pooled)\n",
        "\n",
        "        if return_physics:\n",
        "            # 2. Î¨ºÎ¶¨ Ìó§Îìú (Î≥¥Ï°∞ ÌÉúÏä§ÌÅ¨)\n",
        "            # [B, C, L] -> [B, L, C] -> [B, L, 6]\n",
        "            x_feat_transposed = x_feat.transpose(1, 2)\n",
        "            physics = self.physics_head(x_feat_transposed)\n",
        "            return logits, physics\n",
        "\n",
        "        return logits\n",
        "\n",
        "# ========================\n",
        "# 'Î¨ºÎ¶¨ ÏÜêÏã§' Ìï®Ïàò\n",
        "# ========================\n",
        "def physics_loss(physics_pred, X_raw):\n",
        "    # physics_pred: [B, L, 6]\n",
        "    # X_raw: [B, L, 9] (ÏõêÎ≥∏ ÏûÖÎ†•)\n",
        "\n",
        "    acc_pred = physics_pred[:, :, :3]\n",
        "    gyro_pred = physics_pred[:, :, 3:6]\n",
        "\n",
        "    acc_true = X_raw[:, :, 0:3]\n",
        "    gyro_true = X_raw[:, :, 3:6]\n",
        "\n",
        "    return F.smooth_l1_loss(acc_pred, acc_true) + F.smooth_l1_loss(gyro_pred, gyro_true)\n",
        "\n",
        "# ========================\n",
        "# 'Î¨ºÎ¶¨ Í∏∞Î∞ò' ÌïôÏäµ Ìï®Ïàò\n",
        "# ========================\n",
        "def train_physics(model, train_loader, test_loader, device, epochs=50):\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.01)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
        "\n",
        "    best_acc = 0\n",
        "\n",
        "    print(\"=\"*50)\n",
        "    print(\"üî• Physics-Guided (Multi-scale Modern TCN) HAR ÌïôÏäµ ÏãúÏûë\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss, correct, total = 0, 0, 0\n",
        "\n",
        "        for X, y, _ in train_loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            logits, physics = model(X, return_physics=True)\n",
        "\n",
        "            loss_cls = F.cross_entropy(logits, y)\n",
        "            loss_phys = physics_loss(physics, X)\n",
        "            loss = loss_cls + 0.05 * loss_phys\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            correct += (logits.argmax(1) == y).sum().item()\n",
        "            total += y.size(0)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        model.eval()\n",
        "        test_correct, test_total = 0, 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for X, y, _ in test_loader:\n",
        "                X, y = X.to(device), y.to(device)\n",
        "                logits = model(X, return_physics=False)\n",
        "                test_correct += (logits.argmax(1) == y).sum().item()\n",
        "                test_total += y.size(0)\n",
        "\n",
        "        train_acc = 100 * correct / total\n",
        "        test_acc = 100 * test_correct / test_total\n",
        "        best_acc = max(best_acc, test_acc)\n",
        "\n",
        "        if (epoch + 1) % 10 == 0 or epoch == epochs - 1:\n",
        "            print(f'[Physics] Epoch {epoch+1:02d}/{epochs}: Train Acc={train_acc:.2f}%, Test Acc={test_acc:.2f}% (Best: {best_acc:.2f}%)')\n",
        "\n",
        "    return best_acc\n",
        "\n",
        "# ========================\n",
        "# Î©îÏù∏ Ïã§Ìñâ Ìï®Ïàò\n",
        "# ========================\n",
        "def main():\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # !!! Í≤ΩÎ°ú ÏàòÏ†ï ÌïÑÏöî !!!\n",
        "    data_path = '/content/drive/MyDrive/Colab Notebooks/UCI-HAR/UCI-HAR'\n",
        "    # data_path = './UCI-HAR' # Î°úÏª¨ ÌôòÍ≤Ω ÏòàÏãú\n",
        "\n",
        "    try:\n",
        "        train_ds = UCIHARDataset(data_path, split='train')\n",
        "        test_ds = UCIHARDataset(data_path, split='test')\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Îç∞Ïù¥ÌÑ∞ÏÖãÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§. 'data_path' Î≥ÄÏàòÎ•º ÏàòÏ†ïÌïòÏÑ∏Ïöî.\")\n",
        "        print(f\"ÌòÑÏû¨ Í≤ΩÎ°ú: {data_path}\")\n",
        "        return\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=64, shuffle=True, num_workers=2)\n",
        "    test_loader = DataLoader(test_ds, batch_size=64, shuffle=False, num_workers=2)\n",
        "\n",
        "    print(f\"Train: {len(train_ds)} samples, Test: {len(test_ds)} samples\\n\")\n",
        "\n",
        "    # üî• Physics-Guided Multi-scale Modern TCN Î™®Îç∏ ÏÉùÏÑ± Î∞è ÌïôÏäµ\n",
        "    model_physics = PhysicsModernTCNHAR(\n",
        "        input_dim=9,            # UCI-HAR 9 Ï±ÑÎÑê\n",
        "        hidden_dim=128,         # ÏùÄÎãâÏ∏µ Ï∞®Ïõê\n",
        "        n_layers=4,             # TCN Î∏îÎ°ù Í∞úÏàò\n",
        "        n_classes=6,            # UCI-HAR 6Í∞ú ÌÅ¥ÎûòÏä§\n",
        "        kernel_sizes=[3, 7],    # Multi-scale: ÏûëÏùÄ Ïª§ÎÑê(3) + ÌÅ∞ Ïª§ÎÑê(7)\n",
        "        large_kernel=21,        # Îß§Ïö∞ ÌÅ∞ Ïª§ÎÑê (Í∏ÄÎ°úÎ≤å Ïª®ÌÖçÏä§Ìä∏Ïö©)\n",
        "        dropout=0.1,            # Dropout ÎπÑÏú®\n",
        "        use_se=True             # Squeeze-and-Excitation ÏÇ¨Ïö©\n",
        "    ).to(device)\n",
        "\n",
        "    print(f\"\\nüìä Î™®Îç∏ Íµ¨Ï°∞:\")\n",
        "    print(f\"  - Multi-scale kernels: [3, 7] (ÏûëÏùÄ Ïª§ÎÑê + ÌÅ∞ Ïª§ÎÑê)\")\n",
        "    print(f\"  - Large kernel: 21 (Í∏ÄÎ°úÎ≤å Ïª®ÌÖçÏä§Ìä∏)\")\n",
        "    print(f\"  - Hidden dim: 128\")\n",
        "    print(f\"  - Layers: 4\")\n",
        "    print(f\"  - Squeeze-Excitation: Enabled\")\n",
        "    print(f\"  - Total parameters: {sum(p.numel() for p in model_physics.parameters()):,}\\n\")\n",
        "\n",
        "    acc_physics = train_physics(model_physics, train_loader, test_loader, device, epochs=50)\n",
        "\n",
        "    # Summary\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"FINAL RESULT\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"üî• Physics-Guided Multi-scale Modern TCN HAR (Best Test Acc): {acc_physics:.2f}%\")\n",
        "    print(f\"   - Small kernels (3, 7) for local patterns\")\n",
        "    print(f\"   - Large kernel (21) for global context\")\n",
        "    print(f\"   - Physics-guided learning with auxiliary task\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nOj8MceDnsN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VlovXjOVBBj-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zbtj4KFd-3WP",
        "outputId": "55ac2b17-6c4f-46da-a64b-a03a2fda155c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Train: 7352 samples, Test: 2947 samples\n",
            "\n",
            "==================================================\n",
            "5Ô∏è‚É£ Physics-Guided (Transformer) HAR ÌïôÏäµ ÏãúÏûë\n",
            "==================================================\n",
            "[Physics] Epoch 10/50: Train Acc=94.78%, Test Acc=89.11% (Best: 90.43%)\n",
            "[Physics] Epoch 20/50: Train Acc=95.23%, Test Acc=90.43% (Best: 91.41%)\n",
            "[Physics] Epoch 30/50: Train Acc=96.08%, Test Acc=89.51% (Best: 91.41%)\n",
            "[Physics] Epoch 40/50: Train Acc=96.98%, Test Acc=90.67% (Best: 91.55%)\n",
            "[Physics] Epoch 50/50: Train Acc=97.58%, Test Acc=90.43% (Best: 91.55%)\n",
            "\n",
            "==================================================\n",
            "FINAL RESULT\n",
            "==================================================\n",
            "5Ô∏è‚É£ Physics-Guided Transformer HAR (Best Test Acc): 91.55%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pathlib import Path\n",
        "import time\n",
        "from collections import deque\n",
        "import math # PositionalEncodingÏóê ÌïÑÏöî\n",
        "\n",
        "# ========================\n",
        "# UCI-HAR Îç∞Ïù¥ÌÑ∞ Î°úÎìú\n",
        "# ========================\n",
        "class UCIHARDataset(Dataset):\n",
        "    def __init__(self, data_path, split='train'):\n",
        "        base = Path(data_path) / split\n",
        "        signals = []\n",
        "        for sensor in ['body_acc', 'body_gyro', 'total_acc']:\n",
        "            for axis in ['x', 'y', 'z']:\n",
        "                file = base / 'Inertial Signals' / f'{sensor}_{axis}_{split}.txt'\n",
        "                signals.append(np.loadtxt(file))\n",
        "\n",
        "        self.X = np.stack(signals, axis=-1)\n",
        "        self.y = np.loadtxt(base.parent / split / f'y_{split}.txt').astype(int) - 1\n",
        "\n",
        "        try:\n",
        "            self.subjects = np.loadtxt(base.parent / split / f'subject_{split}.txt').astype(int)\n",
        "        except:\n",
        "            self.subjects = np.ones(len(self.y))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return (torch.FloatTensor(self.X[idx]),\n",
        "                torch.LongTensor([self.y[idx]])[0],\n",
        "                self.subjects[idx])\n",
        "\n",
        "# ========================\n",
        "# üî• [Ïã†Í∑ú] Ìä∏ÎûúÏä§Ìè¨Î®∏ Ìè¨ÏßÄÏÖîÎÑê Ïù∏ÏΩîÎî©\n",
        "# ========================\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0) # [1, max_len, d_model]\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: [B, L, D]\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "# ========================\n",
        "# Ìä∏ÎûúÏä§Ìè¨Î®∏ Base Î™®Îç∏\n",
        "# ========================\n",
        "class BaseTransformerHAR(nn.Module):\n",
        "    def __init__(self, input_dim=9, d_model=128, n_head=8, n_layers=4, n_classes=6, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.proj = nn.Linear(input_dim, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=d_model,\n",
        "            nhead=n_head,\n",
        "            dim_feedforward=d_model * 4,\n",
        "            dropout=dropout,\n",
        "            batch_first=True # (B, L, D) ÏûÖÎ†•ÏùÑ ÏúÑÌï®\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(\n",
        "            encoder_layer,\n",
        "            num_layers=n_layers\n",
        "        )\n",
        "\n",
        "        self.norm_final = nn.LayerNorm(d_model)\n",
        "        self.head = nn.Linear(d_model, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Ïù¥ Í∏∞Î≥∏ forwardÎäî Physics Î™®Îç∏ÏóêÏÑú Ïò§Î≤ÑÎùºÏù¥Îìú(override) Îê©ÎãàÎã§.\n",
        "        x = self.proj(x)\n",
        "        x = self.pos_encoder(x)\n",
        "        x = self.transformer_encoder(x)\n",
        "        x = self.norm_final(x)\n",
        "        return self.head(x[:, -1, :]) # ÎßàÏßÄÎßâ ÌÜ†ÌÅ∞Ïùò Ï∂úÎ†•ÏùÑ Î∂ÑÎ•òÏóê ÏÇ¨Ïö©\n",
        "\n",
        "# ========================\n",
        "# Physics-Guided Transformer HAR\n",
        "# ========================\n",
        "class PhysicsTransformerHAR(BaseTransformerHAR):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "        d_model = self.head.in_features\n",
        "        # Î≥¥Ï°∞ ÌÉúÏä§ÌÅ¨Î•º ÏúÑÌïú 'Î¨ºÎ¶¨ Ìó§Îìú' Ï∂îÍ∞Ä\n",
        "        self.physics_head = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_model // 2, 6) # 6 = acc (x,y,z) + gyro (x,y,z)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, return_physics=False):\n",
        "        # BaseTransformerHARÏùò forward Î°úÏßÅ (Transformer Ïù∏ÏΩîÎçî ÌÜµÍ≥º)\n",
        "        x_feat = self.proj(x)\n",
        "        x_feat = self.pos_encoder(x_feat)\n",
        "        x_feat = self.transformer_encoder(x_feat)\n",
        "        x_feat = self.norm_final(x_feat) # (B, L, D_model)\n",
        "\n",
        "        # 1. Î∂ÑÎ•ò Ìó§Îìú (Í∏∞Î≥∏ ÌÉúÏä§ÌÅ¨)\n",
        "        logits = self.head(x_feat[:, -1, :]) # ÎßàÏßÄÎßâ ÌÉÄÏûÑÏä§ÌÖùÏùò ÌîºÏ≤òÎßå ÏÇ¨Ïö©\n",
        "\n",
        "        if return_physics:\n",
        "            # 2. Î¨ºÎ¶¨ Ìó§Îìú (Î≥¥Ï°∞ ÌÉúÏä§ÌÅ¨)\n",
        "            # (B, L, D_model) -> (B, L, 6)\n",
        "            physics = self.physics_head(x_feat) # Î™®Îì† ÌÉÄÏûÑÏä§ÌÖùÏùò ÌîºÏ≤ò ÏÇ¨Ïö©\n",
        "            return logits, physics\n",
        "\n",
        "        return logits\n",
        "\n",
        "# ========================\n",
        "# 'Î¨ºÎ¶¨ ÏÜêÏã§' Ìï®Ïàò (Î≥ÄÍ≤Ω ÏóÜÏùå)\n",
        "# ========================\n",
        "def physics_loss(physics_pred, X_raw):\n",
        "    # physics_pred: [B, L, 6]\n",
        "    # X_raw: [B, L, 9] (ÏõêÎ≥∏ ÏûÖÎ†•)\n",
        "\n",
        "    acc_pred = physics_pred[:, :, :3]\n",
        "    gyro_pred = physics_pred[:, :, 3:6]\n",
        "\n",
        "    acc_true = X_raw[:, :, 0:3]\n",
        "    gyro_true = X_raw[:, :, 3:6]\n",
        "\n",
        "    return F.smooth_l1_loss(acc_pred, acc_true) + F.smooth_l1_loss(gyro_pred, gyro_true)\n",
        "\n",
        "# ========================\n",
        "# 'Î¨ºÎ¶¨ Í∏∞Î∞ò' ÌïôÏäµ Ìï®Ïàò\n",
        "# ========================\n",
        "def train_physics(model, train_loader, test_loader, device, epochs=50):\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.01)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, epochs)\n",
        "\n",
        "    best_acc = 0\n",
        "\n",
        "    print(\"=\"*50)\n",
        "    print(\"5Ô∏è‚É£ Physics-Guided (Transformer) HAR ÌïôÏäµ ÏãúÏûë\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss, correct, total = 0, 0, 0\n",
        "\n",
        "        for X, y, _ in train_loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            logits, physics = model(X, return_physics=True)\n",
        "\n",
        "            loss_cls = F.cross_entropy(logits, y)\n",
        "            loss_phys = physics_loss(physics, X)\n",
        "            loss = loss_cls + 0.05 * loss_phys\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            correct += (logits.argmax(1) == y).sum().item()\n",
        "            total += y.size(0)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        model.eval()\n",
        "        test_correct, test_total = 0, 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for X, y, _ in test_loader:\n",
        "                X, y = X.to(device), y.to(device)\n",
        "                logits = model(X, return_physics=False)\n",
        "                test_correct += (logits.argmax(1) == y).sum().item()\n",
        "                test_total += y.size(0)\n",
        "\n",
        "        train_acc = 100 * correct / total\n",
        "        test_acc = 100 * test_correct / test_total\n",
        "        best_acc = max(best_acc, test_acc)\n",
        "\n",
        "        if (epoch + 1) % 10 == 0 or epoch == epochs - 1:\n",
        "            print(f'[Physics] Epoch {epoch+1:02d}/{epochs}: Train Acc={train_acc:.2f}%, Test Acc={test_acc:.2f}% (Best: {best_acc:.2f}%)')\n",
        "\n",
        "    return best_acc\n",
        "\n",
        "# ========================\n",
        "# Î©îÏù∏ Ïã§Ìñâ Ìï®Ïàò\n",
        "# ========================\n",
        "def main():\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # !!! Í≤ΩÎ°ú ÏàòÏ†ï ÌïÑÏöî !!!\n",
        "    data_path = '/content/drive/MyDrive/Colab Notebooks/UCI-HAR/UCI-HAR'\n",
        "    # data_path = './UCI-HAR' # Î°úÏª¨ ÌôòÍ≤Ω ÏòàÏãú\n",
        "\n",
        "    try:\n",
        "        train_ds = UCIHARDataset(data_path, split='train')\n",
        "        test_ds = UCIHARDataset(data_path, split='test')\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Îç∞Ïù¥ÌÑ∞ÏÖãÏùÑ Ï∞æÏùÑ Ïàò ÏóÜÏäµÎãàÎã§. 'data_path' Î≥ÄÏàòÎ•º ÏàòÏ†ïÌïòÏÑ∏Ïöî.\")\n",
        "        print(f\"ÌòÑÏû¨ Í≤ΩÎ°ú: {data_path}\")\n",
        "        return\n",
        "\n",
        "    train_loader = DataLoader(train_ds, batch_size=64, shuffle=True, num_workers=2)\n",
        "    test_loader = DataLoader(test_ds, batch_size=64, shuffle=False, num_workers=2)\n",
        "\n",
        "    print(f\"Train: {len(train_ds)} samples, Test: {len(test_ds)} samples\\n\")\n",
        "\n",
        "    # 5Ô∏è‚É£ Physics-Guided Î™®Îç∏ ÏÉùÏÑ± Î∞è ÌïôÏäµ (üî• Mamba -> TransformerÎ°ú Î≥ÄÍ≤Ω)\n",
        "    model_physics = PhysicsTransformerHAR(\n",
        "        input_dim=9,    # UCI-HAR 9 Ï±ÑÎÑê\n",
        "        d_model=128,\n",
        "        n_head=8,       # Ìä∏ÎûúÏä§Ìè¨Î®∏ Ìó§Îìú Ïàò\n",
        "        n_layers=4,\n",
        "        n_classes=6     # UCI-HAR 6Í∞ú ÌÅ¥ÎûòÏä§\n",
        "    ).to(device)\n",
        "\n",
        "    acc_physics = train_physics(model_physics, train_loader, test_loader, device, epochs=50)\n",
        "\n",
        "    # Summary\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"FINAL RESULT\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"5Ô∏è‚É£ Physics-Guided Transformer HAR (Best Test Acc): {acc_physics:.2f}%\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yo3zpthg-3Zm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (har-cu126)",
      "language": "python",
      "name": "har-cu126"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
